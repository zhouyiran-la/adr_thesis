### 3.1 边缘AI加速设备硬件架构介绍

RK1808人工智能计算棒是瑞芯微旗下 Toybrick 系列产品之一，其外观尺寸如图3-1所示。RK1808人工智能计算棒搭载 RK1808芯片，RK1808芯片由双核Cortex-A35和内置NPU处理器组成，采用22纳米工艺制造，集成了高速DDR3/LPDDR3内存控制器与大量接口，同时支持多种加密算法，具有低能耗、高性能的特点，能够为RK1808人工智能计算棒提供高效AI推理性能。计算棒内置2GB LPDDR3内存和16GB EMMC存储，提供高速内存访问速度与一定存储空间。在外部接口方面，RK1808计算棒仅包括一个USB3.0 Type-A接口，该接口既用于供电和数据传输，也可连接外部摄像头和其他设备。简单来说，RK1808人工智能计算棒被设计成一个简单USB设备，具有USB接口的边缘计算嵌入式平台可利用计算棒推理性能在网络边缘侧部署人工智能算法，使计算资源受限的嵌入式设备完成人工智能升级。在自主无人机竞速领域，无人机机载电脑通过结合RK1808计算棒能够加速网络模型推理，从而使得深度学习类自主无人机竞速算法在边缘本地部署成为可能。此外，RK1808 人工智能计算棒既可用于辅助推理计算，也支持通过二次开发完成独立人工智能计算功能。需要注意的是，RK1808计算棒只能运行自身RKNN模型文件，故使用RK1808计算棒必须将常规框架（Pytorch/Tensorflow/Caffe）下所导出的网络模型转换为RKNN模型。
\[图3-1]

### 3.2 RK1808计算板卡模型部署方案设计

#### 3.2.1 模型部署整体流程

在本次工作中模型部署具体指使已经过训练的神经网络模型能够在RK1808 AI计算平台中运行，卸载无人机机载电脑（Onboard Computer）模型推理任务，从而使深度学习类竞速算法能够成功部署于无人机设备。RK1808计算板卡支持主动和被动两种开发模式，被动模式下，RK1808作为其上位机的一个外接USB计算设备，被动接收网络模型和输入数据，整个过程中RK1808与上位机数据传输需依靠RKNN-Toolkit完成；主动模式下，RK1808计算棒可视为一款具备AI推理能力的特殊主机，且内部默认预装RKNN-Toolkit与RKNN-API，上位机无需再次安装。其作为主设备，经过配置后能够集成所需网络模型，上位机只需将输入数据（如图片或视频流）通过USB接口输入至计算棒，RK1808计算棒将自动完成模型推理并将输出结果再次通过USB接口返回至上位机。目前被动模式仅支持有限平台，诸如NVIDIA Jetson、Raspberry Pi等常用移动端嵌入式计算平台（包括无人机机载电脑）皆无法使用被动模式开发，而主动模式具有平台无关性，不限制上位机系统架构且可搭载任意操作系统，故后续无人机竞速模型部署流程皆基于主动模式设计。

图3-2展示了单计算棒条件下无人机竞速模型部署整体流程，此时上位机特指无人机机载电脑。基于3.1中内容，RK1808计算板卡只支持运行RKNN模型，故模型部署首先应利用RKNN-Toolkit进行模型转换，将开源深度学习框架导出的模型转成RKNPU所能识别的RKNN模型，其中模型量化是模型转换中的重要步骤，具体细节详见3.2.2。模型转换完成后，需建立机载电脑与RK1808之间的网络连接，具体来说，RK1808计算棒通过USB接口插入机载电脑后，会作为虚拟网卡设备在机载电脑端生成网络节点，机载电脑端需利用网络管理工具对该虚拟网络节点配置IP、子网掩码等基本属性信息，保证机载电脑与RK1808之间能够进行正常网络通信。之后，机载电脑端可以利用上述IP远程登录RK1808计算棒（实际中内置操作系统为Fedora），将所需RKNN模型固化于RK1808。最后，完善模型推理数据流，即RK1808计算棒运行服务端程序以接收网络输入数据并自动进行模型推理；与此同时，机载电脑端运行客户端程序向计算帮发送推理计算请求与网络输入数据，待计算棒完成推理后返回输出结果。由此无人机机载电脑与RK1808人工智能计算棒之间形成了典型CS模式，两者得以相互配合完成复杂网络模型推理任务。
\[图3-2]

#### 3.2.2 模型量化

模型量化是一种深度神经网络模型优化常用技术，其基本原理是建立浮点数与定点数的数据映射关系，从而将网络模型中浮点参数转化为定点参数，减少存储与计算所需资源，。具体来说，量化模型使用较低精度（如 int8/uint8/int16）保存模型权重信息，在部署时可以使用更少存储空间获得更快推理速度。由于各深度学习框架训练、保存模型时通常使用浮点数据，故模型量化是RK1808计算棒模型转换过程中非常重要的一环。RKNN-Toolkit是RK系列计算平台经常使用的模型转换工具，用户可以通过安装RKNN-Toolkit直接调用Python接口完成进行模型转换、推理运行和性能评估等环节。目前RKNN-Toolkit主要支持两种模型量化形式：一种是根据用户提供的量化数据集，对加载的浮点模型进行量化，生成量化RKNN模型；另一种是由深度学习框架导出量化模型，RKNN-Toolkit加载并利用已有的量化信息生成量化RKNN模型。
公式3-1
根据执行量化操作的时间不同，可以将模型量化分为训练后量化（Post-Training Quantization）和量化感知训练（Quantization-Aware Training, QAT），后训练量化是指将预先训练好的浮点模型转换为定点模型。在后训练量化过程中，首先需要对预训练好的浮点模型进行推理，记录模型中权重和激活值的统计信息，并根据这些信息将浮点数转化成定点数，RKNN-Toolkit中可以通过调用config 接口指定特定训练后量化方法与校准数据集。量化感知训练旨在在模型训练过程中考虑后续量化操作，这一过程通常会降低训练数据精度，使模型在低精度数据上进行训练，同时对权重和激活值使用动态或静态量化方法。该种量化方法能够让模型在训练过程中尽可能地学习低精度权重和激活值的特征表示，模型。RKNN Toolkit 目前支持 TensorFlow 和 PyTorch 这两种框架量化感知训练得到的模型。

使用RKNN-Toolkit进行模型量化时可能会导致原有模型精度下降，主要可以归因为以下三种情况：一是RKNN-Toolkit对该层算子的匹配有问题，或者NPU驱动对该算子的实现有问题（此时该层推理结果通常会和浮点模型结果差距巨大，余弦距离小于0.5）；二是该层数据分布对量化操作不友好（例如数据分布范围比较广，用低精度表达后精度损失严重）；三是RKNN-Toolkit或NPU驱动的某些图优化可能导致精度下降，例如用conv替换add或者average pool等。对于第一种场景，可以尝试用混合量化的方式进行规避；对于第二种场景，可以尝试用MMSE或KL散度方法对量化参数进行调优。若还无法改善模型精度，可以考虑使用混合量化，用更高精度方法去计算对量化操作不友好的算子，或者使用量化感知训练直接得到量化模型，此时RKNN-Toolkit将使用模型自身量化参数，几乎不会造成精度损失；对于第三种场景，可以尝试将优化等级下调（设置config接口中optimization_level参数）。
 


